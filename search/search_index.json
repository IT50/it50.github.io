{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"en/","text":"True control comes after knowledge. This book introduces our everyday computer systems from the perspective of a professional software user rather than a service user. You will be learning the fundamentals that a professional software user needs and then have greater control over the software you use. Read the Book What is IT50? IT stands for Information Technology, a phrase less used in the 2020s. The reason behind this decline is the prosperity of services making users ignorant of the technical aspects. In this project, we bring those back to everyone who wishes to know. Our goal comes from the belief that everyone should be able to control their own computer systems as much as possible. To empower that, we aim to give readers a high-level cognition of how things work. This project focuses exclusively on the established concepts in computer software. We are aware of many other introductory guides, and we know why we are special. A professional software user does not have to be a programmer, in the same sense as drivers do not need to know how to make cars. Learning how things work at a certain level is the sweet spot for the vast majority of people, and this guide is built right at this sweet spot. If you are looking for a programming guide, then this is not the right place, although it would be very beneficial to have a look. Indeed, driving is a useful skill, and so is computer literacy. We are sure you are going to find its significance. Contributing There is ongoing discussion about further content and project structure. We are open to accept new contributors and unsolicited commits. To submit contributions, use our GitHub repository .","title":"True control comes after knowledge."},{"location":"en/#true-control-comes-after-knowledge","text":"This book introduces our everyday computer systems from the perspective of a professional software user rather than a service user. You will be learning the fundamentals that a professional software user needs and then have greater control over the software you use. Read the Book","title":"True control comes after knowledge."},{"location":"en/#what-is-it50","text":"IT stands for Information Technology, a phrase less used in the 2020s. The reason behind this decline is the prosperity of services making users ignorant of the technical aspects. In this project, we bring those back to everyone who wishes to know. Our goal comes from the belief that everyone should be able to control their own computer systems as much as possible. To empower that, we aim to give readers a high-level cognition of how things work. This project focuses exclusively on the established concepts in computer software. We are aware of many other introductory guides, and we know why we are special. A professional software user does not have to be a programmer, in the same sense as drivers do not need to know how to make cars. Learning how things work at a certain level is the sweet spot for the vast majority of people, and this guide is built right at this sweet spot. If you are looking for a programming guide, then this is not the right place, although it would be very beneficial to have a look. Indeed, driving is a useful skill, and so is computer literacy. We are sure you are going to find its significance.","title":"What is IT50?"},{"location":"en/#contributing","text":"There is ongoing discussion about further content and project structure. We are open to accept new contributors and unsolicited commits. To submit contributions, use our GitHub repository .","title":"Contributing"},{"location":"en/asymmetric-cryptography/","text":"Asymmetric Cryptography Asymmetric cryptography has a pair of keys. Two keys are generated at once using a mathematical technique so that the two keys are not inferable from one to the other, and when one of the keys is used to encrypt data, only the other key can be used to decrypt. The owner of the key pair generates a pair of keys. Then, one key is made public ( public key ), and the other is kept private ( private key ). Now the following actions are possible: Sending a message securely to a designated recipient. Transforming plaintext with the public key ensures that the data can only be read by the designated recipient, the owner of the private key. Proving authenticity. Transforming a piece of data with the private key proves the authenticity of the data because only the owner of the key can do so. Thus, this action is not called encrypting but signing as it does not make anything secret. In many contexts, signing implies endorsement of the content. Ideal workflow for sending a message Everyone knows everyone\u2019s public key. Alice has the plaintext. Alice encrypts the plaintext with Bob\u2019s public key. Alice sends the ciphertext to Bob. Bob decrypts the data with his private key. Bob has the plaintext. Note that Alice does not encrypt with her own private key; that would be signing a publication. However, in the real world it is not practical to know everyone\u2019s public key in advance. A trusted channel is required to pre-distribute the public key, or the key has to be shared at the start of the connection. In the second case, the connection bearer can always perform a man-in-the-middle ( MITM ) attack. The next section shows how such attack works. Workflow for sending a message, but with in-time public key sharing and MITM attack Alice has the plaintext. Alice requests Bob for his public key and this request is intercepted by Charles. Charles requests Bob for his public key. Bob thinks Charles were Alice. Bob responds with his public key. Charles responds Alice with his own public key. Alice thinks Charles were Bob. Alice encrypts the plaintext with Charles\u2019 public key. Alice sends the ciphertext to Bob and it is intercepted by Charles. Charles decrypts the ciphertext with his private key. Charles has the plaintext. Charles optionally modifies the plaintext. Charles encrypts the plaintext with Bob\u2019s public key. Bob decrypts the ciphertext with his private key. Bob has the (possibly modified) plaintext. Note that in order to perform an MITM attack, the attacker must be able to intercept and modify all messages between Alice and Bob. Merely viewing public keys does not compromise asymmetric cryptography. The world currently has a widely used (and pretty solid) way to distribute authentic public keys, and we will talk about that in the Public Key Infrastructure chapter. If this technique is used, Alice should have noticed that the public key is not authentic. The two scenarios above also apply to signing and signature verification. For brevity, only the ideal one is detailed. Ideal workflow for signing and signature verification Alice has a piece of data to publish. Alice signs (transforms) a cryptographic hash of the data with her private key. This is the signature . Alice sends the original data along with the signature. Bob receives the data along with the signature. Bob calculates the hash of the data. Bob transforms the signature with Alice\u2019s public key, and gets the authentic hash of the data. If the hashes match, the signature is verified and the data is authentic. Else, the data or the signature has changed.","title":"Asymmetric Cryptography"},{"location":"en/asymmetric-cryptography/#asymmetric-cryptography","text":"Asymmetric cryptography has a pair of keys. Two keys are generated at once using a mathematical technique so that the two keys are not inferable from one to the other, and when one of the keys is used to encrypt data, only the other key can be used to decrypt. The owner of the key pair generates a pair of keys. Then, one key is made public ( public key ), and the other is kept private ( private key ). Now the following actions are possible: Sending a message securely to a designated recipient. Transforming plaintext with the public key ensures that the data can only be read by the designated recipient, the owner of the private key. Proving authenticity. Transforming a piece of data with the private key proves the authenticity of the data because only the owner of the key can do so. Thus, this action is not called encrypting but signing as it does not make anything secret. In many contexts, signing implies endorsement of the content.","title":"Asymmetric Cryptography"},{"location":"en/asymmetric-cryptography/#ideal-workflow-for-sending-a-message","text":"Everyone knows everyone\u2019s public key. Alice has the plaintext. Alice encrypts the plaintext with Bob\u2019s public key. Alice sends the ciphertext to Bob. Bob decrypts the data with his private key. Bob has the plaintext. Note that Alice does not encrypt with her own private key; that would be signing a publication. However, in the real world it is not practical to know everyone\u2019s public key in advance. A trusted channel is required to pre-distribute the public key, or the key has to be shared at the start of the connection. In the second case, the connection bearer can always perform a man-in-the-middle ( MITM ) attack. The next section shows how such attack works.","title":"Ideal workflow for sending a message"},{"location":"en/asymmetric-cryptography/#workflow-for-sending-a-message-but-with-in-time-public-key-sharing-and-mitm-attack","text":"Alice has the plaintext. Alice requests Bob for his public key and this request is intercepted by Charles. Charles requests Bob for his public key. Bob thinks Charles were Alice. Bob responds with his public key. Charles responds Alice with his own public key. Alice thinks Charles were Bob. Alice encrypts the plaintext with Charles\u2019 public key. Alice sends the ciphertext to Bob and it is intercepted by Charles. Charles decrypts the ciphertext with his private key. Charles has the plaintext. Charles optionally modifies the plaintext. Charles encrypts the plaintext with Bob\u2019s public key. Bob decrypts the ciphertext with his private key. Bob has the (possibly modified) plaintext. Note that in order to perform an MITM attack, the attacker must be able to intercept and modify all messages between Alice and Bob. Merely viewing public keys does not compromise asymmetric cryptography. The world currently has a widely used (and pretty solid) way to distribute authentic public keys, and we will talk about that in the Public Key Infrastructure chapter. If this technique is used, Alice should have noticed that the public key is not authentic. The two scenarios above also apply to signing and signature verification. For brevity, only the ideal one is detailed.","title":"Workflow for sending a message, but with in-time public key sharing and MITM attack"},{"location":"en/asymmetric-cryptography/#ideal-workflow-for-signing-and-signature-verification","text":"Alice has a piece of data to publish. Alice signs (transforms) a cryptographic hash of the data with her private key. This is the signature . Alice sends the original data along with the signature. Bob receives the data along with the signature. Bob calculates the hash of the data. Bob transforms the signature with Alice\u2019s public key, and gets the authentic hash of the data. If the hashes match, the signature is verified and the data is authentic. Else, the data or the signature has changed.","title":"Ideal workflow for signing and signature verification"},{"location":"en/binary-data/","text":"Binary Data It is commonly known that computers hold and process data in binary \u2014 0\u2019s and 1\u2019s. However, it may take a few moments for you to understand what that actually means. To illustrate some points, we start with an example: 0000011000001000000010110100010011100010101010101010101101001101 Bit and Byte A single digit in the data above represents a bit . Its symbol is a lowercase b (or for better discrimination, bit ). A bit has only two possible values, \u201coff\u201d and \u201con\u201d, and we use 0 and 1 to represent them. The data above contains 64 bits. A byte is 8 bits long. Its symbol is an uppercase B . Today's computers know bytes inherently. Computers process a multitude of bytes at once. They also recognize the positions where bytes start. Bytes are usually only used as a whole. Incomplete bytes do not exist. The example data are these bytes: 00000110 00001000 00001011 01000100 11100010 10101010 10101011 01001101 and there is no such thing as: 00 00011000 00100000 00101101 00010011 10001010 10101010 10101101 001101 or 00000110 00001000 00001011 01000100 11100010 10101010 10101011 01001 You will also read data in bytes rather than in bits. A byte is commonly written as a two-digit hexadecimal number that can be directly converted from its binary representation. For example, 0b11100100 will be represented as 0xE4 , where in a certain convention 0b stands for binary and 0x stands for hexadecimal. Addressing Similar to how only whole bytes are useful, in certain contexts, only whole blocks of some bytes are useful. In such contexts, the total length of raw data is a multitude of a block size, which is commonly chosen to be a power of 2 bytes. In contexts where the concept of block is not helpful, the length of binary data can be any number of bytes. An address is a number used to locate a byte or block in raw data. Linear addressing is a very simple way to give addresses to positions in a piece of data. The starting position has address 0, and the address increases by 1 for every subsequent byte or block. In our example, the author dictates to not use blocks, so addresses are given to bytes. These are the addresses and their corresponding data. Byte address Data (binary) Data (hexadecimal) 0 00000110 06 1 00001000 08 2 00001011 0B 3 01000100 44 4 11100010 E2 5 10101010 AA 6 10101011 AB 7 01001101 4D It is more than a convention that we count the address and many things else from zero. Look at the table of a few numbers in their decimal and binary forms to find out why. Decimal Binary 0 00000000 1 00000001 63 00111111 2\u2076 = 64 01000000 255 11111111 2\u2078 = 256 100000000 Note where the binary number bumps to the 7th and 9th place. Because there are so many advantages to keep the length of data the same, we want to use all the 2\u2078 = 256 possible values in an 8-bit piece of data. We have to use this combination of all zeros in order to use all of the 256 combinations. Binary data operations In most contexts, binary data has a fixed size and does not become longer or shorter. On binary data, to write means to overwrite, changing a part to the desired content and destroying existing data. The time needed to read and write sequentially on any storage device is proportional to the data size. When the original data is not important, a piece of binary data may be referred to as space. Units The base units for the amount of data are byte and bit. We use prefixes on them to specify a magnitude. As per IEC 80000-13 and SI : Prefix Magnitude (calculated) Prefix Magnitude Ki 2\u00b9\u2070 = 1 024 k 10\u00b3 Mi 2\u00b2\u2070 = 1 048 576 M 10\u2076 Gi 2\u00b3\u2070 = 1 073 741 824 G 10\u2079 Ti 2\u2074\u2070 = 1 099 511 627 776 T 10\u00b9\u00b2 Pi 2\u2075\u2070 \u2248 1.125 899 907 \u00d7 10\u00b9\u2075 P 10\u00b9\u2075 Ei 2\u2076\u2070 \u2248 1.152 921 505 \u00d7 10\u00b9\u2078 E 10\u00b9\u2078 Zi 2\u2077\u2070 \u2248 1.180 591 621 \u00d7 10\u00b2\u00b9 Z 10\u00b2\u00b9 Yi 2\u2078\u2070 \u2248 1.208 925 820 \u00d7 10\u00b2\u2074 Y 10\u00b2\u2074 However, some systems do not use these standards, and that often causes confusion. It is always good to clarify and test before doing things formally. Try it You can view any file and any part of a disk with a binary data viewer. Try a hexadecimal data editor. It allows you to see raw data bytes in hexadecimal. However, you are not able to understand it before you learn the standards, and you will probably corrupt your files if you edit randomly. Remember to back up your files before editing them. Do not edit your disk directly unless you have read the Partitioning and Filesystem chapter.","title":"Binary Data"},{"location":"en/binary-data/#binary-data","text":"It is commonly known that computers hold and process data in binary \u2014 0\u2019s and 1\u2019s. However, it may take a few moments for you to understand what that actually means. To illustrate some points, we start with an example: 0000011000001000000010110100010011100010101010101010101101001101","title":"Binary Data"},{"location":"en/binary-data/#bit-and-byte","text":"A single digit in the data above represents a bit . Its symbol is a lowercase b (or for better discrimination, bit ). A bit has only two possible values, \u201coff\u201d and \u201con\u201d, and we use 0 and 1 to represent them. The data above contains 64 bits. A byte is 8 bits long. Its symbol is an uppercase B . Today's computers know bytes inherently. Computers process a multitude of bytes at once. They also recognize the positions where bytes start. Bytes are usually only used as a whole. Incomplete bytes do not exist. The example data are these bytes: 00000110 00001000 00001011 01000100 11100010 10101010 10101011 01001101 and there is no such thing as: 00 00011000 00100000 00101101 00010011 10001010 10101010 10101101 001101 or 00000110 00001000 00001011 01000100 11100010 10101010 10101011 01001 You will also read data in bytes rather than in bits. A byte is commonly written as a two-digit hexadecimal number that can be directly converted from its binary representation. For example, 0b11100100 will be represented as 0xE4 , where in a certain convention 0b stands for binary and 0x stands for hexadecimal.","title":"Bit and Byte"},{"location":"en/binary-data/#addressing","text":"Similar to how only whole bytes are useful, in certain contexts, only whole blocks of some bytes are useful. In such contexts, the total length of raw data is a multitude of a block size, which is commonly chosen to be a power of 2 bytes. In contexts where the concept of block is not helpful, the length of binary data can be any number of bytes. An address is a number used to locate a byte or block in raw data. Linear addressing is a very simple way to give addresses to positions in a piece of data. The starting position has address 0, and the address increases by 1 for every subsequent byte or block. In our example, the author dictates to not use blocks, so addresses are given to bytes. These are the addresses and their corresponding data. Byte address Data (binary) Data (hexadecimal) 0 00000110 06 1 00001000 08 2 00001011 0B 3 01000100 44 4 11100010 E2 5 10101010 AA 6 10101011 AB 7 01001101 4D It is more than a convention that we count the address and many things else from zero. Look at the table of a few numbers in their decimal and binary forms to find out why. Decimal Binary 0 00000000 1 00000001 63 00111111 2\u2076 = 64 01000000 255 11111111 2\u2078 = 256 100000000 Note where the binary number bumps to the 7th and 9th place. Because there are so many advantages to keep the length of data the same, we want to use all the 2\u2078 = 256 possible values in an 8-bit piece of data. We have to use this combination of all zeros in order to use all of the 256 combinations.","title":"Addressing"},{"location":"en/binary-data/#binary-data-operations","text":"In most contexts, binary data has a fixed size and does not become longer or shorter. On binary data, to write means to overwrite, changing a part to the desired content and destroying existing data. The time needed to read and write sequentially on any storage device is proportional to the data size. When the original data is not important, a piece of binary data may be referred to as space.","title":"Binary data operations"},{"location":"en/binary-data/#units","text":"The base units for the amount of data are byte and bit. We use prefixes on them to specify a magnitude. As per IEC 80000-13 and SI : Prefix Magnitude (calculated) Prefix Magnitude Ki 2\u00b9\u2070 = 1 024 k 10\u00b3 Mi 2\u00b2\u2070 = 1 048 576 M 10\u2076 Gi 2\u00b3\u2070 = 1 073 741 824 G 10\u2079 Ti 2\u2074\u2070 = 1 099 511 627 776 T 10\u00b9\u00b2 Pi 2\u2075\u2070 \u2248 1.125 899 907 \u00d7 10\u00b9\u2075 P 10\u00b9\u2075 Ei 2\u2076\u2070 \u2248 1.152 921 505 \u00d7 10\u00b9\u2078 E 10\u00b9\u2078 Zi 2\u2077\u2070 \u2248 1.180 591 621 \u00d7 10\u00b2\u00b9 Z 10\u00b2\u00b9 Yi 2\u2078\u2070 \u2248 1.208 925 820 \u00d7 10\u00b2\u2074 Y 10\u00b2\u2074 However, some systems do not use these standards, and that often causes confusion. It is always good to clarify and test before doing things formally.","title":"Units"},{"location":"en/binary-data/#try-it","text":"You can view any file and any part of a disk with a binary data viewer. Try a hexadecimal data editor. It allows you to see raw data bytes in hexadecimal. However, you are not able to understand it before you learn the standards, and you will probably corrupt your files if you edit randomly. Remember to back up your files before editing them. Do not edit your disk directly unless you have read the Partitioning and Filesystem chapter.","title":"Try it"},{"location":"en/cryptographic-hash/","text":"Cryptographic Hash Hash function A hash function is a function that takes in data of any length and outputs data of a fixed length, called its hash . Hash functions are deterministic. With the same hash function, the same input data will always result in the same hash. For a good hash function, the time needed to compute grows with the size of the data, because the hash is determined by all of the data. Cryptographic hash functions have this property. Below are two pieces of data and their 256-bit SHA-2 hash. Data (hexadecimal) Text (UTF-8) SHA-2 256 hash (empty) (empty) e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 48656c6c6f20776f726c64 Hello world 64ec88ca00b268e5ba1a35678a1b5316d212f4f366b2477232534a8aeca37f3c Because there are infinite possible inputs but only finitely many hash values, there must be infinite pieces of data that hashes to the same value. This is called collision . Cryptographic hash function A cryptographic hash function is also: Irreversible. The only way to find possible inputs from a hash is to try inputs one by one and check if the hash matched. It is also infeasible to make any inference about the input data from the hash alone. Chaotic. The mapping from data to hashes should seem random. Two similar pieces of data will result in seemingly uncorrelated hashes. Therefore, it is infeasible to construct an input that hashes to a given value (purposefully construct a collision). Data (hexadecimal) SHA-2 256 hash 089f8954a605c945 e7ad94f8f1338ce71c0b033d881abafd978cb332a56f1fe2d79082565aafc5ee 089f8954a605c946 581c858eba8d0c8a9e418f35193ca72536f91f55a7610b7001d0c0330bc8b1b6 Hashing is useful when we need to make sure two pieces of data are the same without comparing any meaningful details in them, often because any meaningful details are secrets or the data is too large to send. When it comes to data validation, systems often compare hash values, not the data themselves. Currently, the most commonly used and secure cryptographic hash functions are the SHA-2 and SHA-3 families. Insecurities When we use cryptographic hashes to secure things, we rely on their irreversibility. Because ideal cryptographic hash functions are irreversible, the only way to attack is to use brute force, simply trying inputs one by one. The chance of finding the same hash value is so low that it is expected to take very long to find the same hash. Even if one is found, the data may not be the original one, because infinite pieces of data hash to the same value. Therefore, for short data such as passwords, we should consider the possibility of a brute force attack trying to find its plain text. The countermeasure is to use a slower algorithm or more iterations, use different salts, and very importantly, as a user, use long, complex passwords and not use a single pattern for all passwords. An intelligent finding in the algorithm may make attacking faster than brute force. When such attacks are found, the hash function should be obsoleted.","title":"Cryptographic Hash"},{"location":"en/cryptographic-hash/#cryptographic-hash","text":"","title":"Cryptographic Hash"},{"location":"en/cryptographic-hash/#hash-function","text":"A hash function is a function that takes in data of any length and outputs data of a fixed length, called its hash . Hash functions are deterministic. With the same hash function, the same input data will always result in the same hash. For a good hash function, the time needed to compute grows with the size of the data, because the hash is determined by all of the data. Cryptographic hash functions have this property. Below are two pieces of data and their 256-bit SHA-2 hash. Data (hexadecimal) Text (UTF-8) SHA-2 256 hash (empty) (empty) e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 48656c6c6f20776f726c64 Hello world 64ec88ca00b268e5ba1a35678a1b5316d212f4f366b2477232534a8aeca37f3c Because there are infinite possible inputs but only finitely many hash values, there must be infinite pieces of data that hashes to the same value. This is called collision .","title":"Hash function"},{"location":"en/cryptographic-hash/#cryptographic-hash-function","text":"A cryptographic hash function is also: Irreversible. The only way to find possible inputs from a hash is to try inputs one by one and check if the hash matched. It is also infeasible to make any inference about the input data from the hash alone. Chaotic. The mapping from data to hashes should seem random. Two similar pieces of data will result in seemingly uncorrelated hashes. Therefore, it is infeasible to construct an input that hashes to a given value (purposefully construct a collision). Data (hexadecimal) SHA-2 256 hash 089f8954a605c945 e7ad94f8f1338ce71c0b033d881abafd978cb332a56f1fe2d79082565aafc5ee 089f8954a605c946 581c858eba8d0c8a9e418f35193ca72536f91f55a7610b7001d0c0330bc8b1b6 Hashing is useful when we need to make sure two pieces of data are the same without comparing any meaningful details in them, often because any meaningful details are secrets or the data is too large to send. When it comes to data validation, systems often compare hash values, not the data themselves. Currently, the most commonly used and secure cryptographic hash functions are the SHA-2 and SHA-3 families.","title":"Cryptographic hash function"},{"location":"en/cryptographic-hash/#insecurities","text":"When we use cryptographic hashes to secure things, we rely on their irreversibility. Because ideal cryptographic hash functions are irreversible, the only way to attack is to use brute force, simply trying inputs one by one. The chance of finding the same hash value is so low that it is expected to take very long to find the same hash. Even if one is found, the data may not be the original one, because infinite pieces of data hash to the same value. Therefore, for short data such as passwords, we should consider the possibility of a brute force attack trying to find its plain text. The countermeasure is to use a slower algorithm or more iterations, use different salts, and very importantly, as a user, use long, complex passwords and not use a single pattern for all passwords. An intelligent finding in the algorithm may make attacking faster than brute force. When such attacks are found, the hash function should be obsoleted.","title":"Insecurities"},{"location":"en/cryptography/","text":"Cryptography Magic? You can say that. The following activities are all made possible by modern cryptography. Ensuring two pieces of data are the same without comparing any meaningful details. Encrypted storage. Sending a message privately to a designated recipient. Proving the authenticity of a publication. Encryption is a transformation where plaintext is losslessly transformed into ciphertext , a form that appears to be random. The opposite of that action is decryption . A key is a parameter used to transform the data. With different keys, ciphertext will be different. A brute force attack means trying parameters one by one until a particular one is found. The security of cryptography is entirely based on the humble idea to make brute force attack astronomically more expensive than the \u201clegitimate\u201d usage. The amount of trials increases exponentially with the key size or hash length. With a sufficient key size or hash length, brute force attack is by far the least prominent insecurity and generally negligible.","title":"Overview"},{"location":"en/cryptography/#cryptography","text":"Magic? You can say that. The following activities are all made possible by modern cryptography. Ensuring two pieces of data are the same without comparing any meaningful details. Encrypted storage. Sending a message privately to a designated recipient. Proving the authenticity of a publication. Encryption is a transformation where plaintext is losslessly transformed into ciphertext , a form that appears to be random. The opposite of that action is decryption . A key is a parameter used to transform the data. With different keys, ciphertext will be different. A brute force attack means trying parameters one by one until a particular one is found. The security of cryptography is entirely based on the humble idea to make brute force attack astronomically more expensive than the \u201clegitimate\u201d usage. The amount of trials increases exponentially with the key size or hash length. With a sufficient key size or hash length, brute force attack is by far the least prominent insecurity and generally negligible.","title":"Cryptography"},{"location":"en/data/","text":"Data Data, information, file... These concepts seems similar. The key to understanding concepts about data is that, low-level data can be abstracted into high-level data when there is context. There can be multiple layers of such abstraction. The lowest level, as you have probably guessed, is binary data. The data forms we directly interact with are typically at higher levels of abstraction. When a piece of data does not have any known context so it can only be read as-is without any understanding, it is called raw . This term can be used at any level of abstraction. In the following chapters, you are going to learn about binary data and the most fundamental standards for such abstraction.","title":"Overview"},{"location":"en/data/#data","text":"Data, information, file... These concepts seems similar. The key to understanding concepts about data is that, low-level data can be abstracted into high-level data when there is context. There can be multiple layers of such abstraction. The lowest level, as you have probably guessed, is binary data. The data forms we directly interact with are typically at higher levels of abstraction. When a piece of data does not have any known context so it can only be read as-is without any understanding, it is called raw . This term can be used at any level of abstraction. In the following chapters, you are going to learn about binary data and the most fundamental standards for such abstraction.","title":"Data"},{"location":"en/file-format/","text":"File Format We just learned about the standards for encoding and decoding text. Another aspect within which they should be consistent is how to save and read various kinds of files. Essentially, they are also just standards built on top of binary data, and the standard for the structure of a file is called its format . This chapter only introduces the most cognitively constructive formats. File name and extensions The file name, including any extensions, is independent from the data in the file \u2014 renaming will not change the data in the file. The extension is only a conventional, supplementary description of the file format, and may or may not provide more detailed description beyond the standardized characteristics in the file. Different formats can even share the same extension. You can open any file with any program and the program should check if the file opened is in a supported format, but a poorly made program may not do so and corrupt the file when saving. Some operating systems solely rely on the extension to pick a program to open a file. Therefore, these operating systems may or may not open the correct program when you try to open a file by clicking on it. List of generic file types, formats and conventional extensions Uncompressed archive of files ( .tar , .zip , etc. that reflect the archive format) Compressed file ( .gz , .xz , .7z , etc. that reflect the compression format) Compressed archive of files ( .zip , .7z , etc. that reflect the format) Binary executable Entry program (no extension on UNIX-like, .exe on Windows) Library program ( .so , .o , .dll on Windows) Raw binary Raw partition or disk image ( .img , .bin ) Memory dump Plain text of any encoding ( .txt ) Program source code ( .c , .cpp , .java , etc. that reflect the language) Shell script ( .sh , .ps1 , etc. that reflect the shell, or no extension on UNIX-like) Markup language document ( .html , .md , etc. that reflect the language) Structured information ( .json , .ini , .yml , etc. that reflect the language) Configuration file of standardized or non-standardized structure ( .conf , .cf , etc. ) Other specialized formats Description of specialized types Archive and compression Archive formats combine multiple files into one file. File compression algorithms can losslessly convert data to and from a less space-consuming form stored in a specialized format. An archive does not have to compress its contents. The UNIX Tape Archive (TAR) format is an example of this. People usually compress the TAR file with one of the compression standards and get the compressed file with double extensions ( .tar.gz ). Other archive formats may have compression integrated and produce one compressed file with one extension. Media Media file formats are not different from the general definition. However, these file formats are containers that contain metadata and the actual media data. The actual media data is encoded with a codec designed for its media type. Media codec is already a complicated topic on its own, but for the purpose of this book, you only need to know the following: Lossless codecs , much like file compression, do lossless conversion between data and analog media. However, they are much more efficient than universal file compression because they are specialized for a particular media type. Examples include Portable Network Graphic (PNG) and Free Lossless Audio Codec (FLAC). Lossy codecs lose information when the media is encoded. They substantially decrease the data sizes while only losing the minimal details that we unlikely see or hear, and the quality is usually configurable in the encoder. Examples include nearly all modern video codecs, JPEG, WebP, MP3 and opus. Try it Create a Microsoft Word (2007 or later) document. You can open it with Microsoft Word, of course. But did you know that it is just a ZIP archive of its contents? Try to open the file with any archive program and you should see the content, which is encoded in a way that Microsoft Word can understand. You may also be able to understand those code. [[IMPL]] Change the extension on a file and use the properties menu to explore how your operating system chooses a program to open a file when you click on it.","title":"File Format"},{"location":"en/file-format/#file-format","text":"We just learned about the standards for encoding and decoding text. Another aspect within which they should be consistent is how to save and read various kinds of files. Essentially, they are also just standards built on top of binary data, and the standard for the structure of a file is called its format . This chapter only introduces the most cognitively constructive formats.","title":"File Format"},{"location":"en/file-format/#file-name-and-extensions","text":"The file name, including any extensions, is independent from the data in the file \u2014 renaming will not change the data in the file. The extension is only a conventional, supplementary description of the file format, and may or may not provide more detailed description beyond the standardized characteristics in the file. Different formats can even share the same extension. You can open any file with any program and the program should check if the file opened is in a supported format, but a poorly made program may not do so and corrupt the file when saving. Some operating systems solely rely on the extension to pick a program to open a file. Therefore, these operating systems may or may not open the correct program when you try to open a file by clicking on it.","title":"File name and extensions"},{"location":"en/file-format/#list-of-generic-file-types-formats-and-conventional-extensions","text":"Uncompressed archive of files ( .tar , .zip , etc. that reflect the archive format) Compressed file ( .gz , .xz , .7z , etc. that reflect the compression format) Compressed archive of files ( .zip , .7z , etc. that reflect the format) Binary executable Entry program (no extension on UNIX-like, .exe on Windows) Library program ( .so , .o , .dll on Windows) Raw binary Raw partition or disk image ( .img , .bin ) Memory dump Plain text of any encoding ( .txt ) Program source code ( .c , .cpp , .java , etc. that reflect the language) Shell script ( .sh , .ps1 , etc. that reflect the shell, or no extension on UNIX-like) Markup language document ( .html , .md , etc. that reflect the language) Structured information ( .json , .ini , .yml , etc. that reflect the language) Configuration file of standardized or non-standardized structure ( .conf , .cf , etc. ) Other specialized formats","title":"List of generic file types, formats and conventional extensions"},{"location":"en/file-format/#description-of-specialized-types","text":"","title":"Description of specialized types"},{"location":"en/file-format/#archive-and-compression","text":"Archive formats combine multiple files into one file. File compression algorithms can losslessly convert data to and from a less space-consuming form stored in a specialized format. An archive does not have to compress its contents. The UNIX Tape Archive (TAR) format is an example of this. People usually compress the TAR file with one of the compression standards and get the compressed file with double extensions ( .tar.gz ). Other archive formats may have compression integrated and produce one compressed file with one extension.","title":"Archive and compression"},{"location":"en/file-format/#media","text":"Media file formats are not different from the general definition. However, these file formats are containers that contain metadata and the actual media data. The actual media data is encoded with a codec designed for its media type. Media codec is already a complicated topic on its own, but for the purpose of this book, you only need to know the following: Lossless codecs , much like file compression, do lossless conversion between data and analog media. However, they are much more efficient than universal file compression because they are specialized for a particular media type. Examples include Portable Network Graphic (PNG) and Free Lossless Audio Codec (FLAC). Lossy codecs lose information when the media is encoded. They substantially decrease the data sizes while only losing the minimal details that we unlikely see or hear, and the quality is usually configurable in the encoder. Examples include nearly all modern video codecs, JPEG, WebP, MP3 and opus.","title":"Media"},{"location":"en/file-format/#try-it","text":"Create a Microsoft Word (2007 or later) document. You can open it with Microsoft Word, of course. But did you know that it is just a ZIP archive of its contents? Try to open the file with any archive program and you should see the content, which is encoded in a way that Microsoft Word can understand. You may also be able to understand those code. [[IMPL]] Change the extension on a file and use the properties menu to explore how your operating system chooses a program to open a file when you click on it.","title":"Try it"},{"location":"en/introduction/","text":"Introduction Organization of this book Based on how the industry is built, this book organizes information into four levels: Concept Including how concepts relate to each other. Standard Describes the standards for the concepts. Most of the standards introduced in this book are open ones that any developer can adapt to. Implementation Describes major implementations of standards. Different implementations often have different functions, performance, and configuration. Application Apply knowledge in the above levels, combining the usage of software. At this level, complex considerations are taken to achieve best practices. Levels can mix in a single chapter. We recommend that you do not skip levels. You will very likely not understand if you still do so. However, if you wish, you can read across chapters skipping higher levels. Be a good learner In the community of professional users, people always learn. In this book, you are only going to learn the fundamentals that enable you to find answers yourself and solve some of the problems you have. That said, do not expect to be a very proficient pro user after reading this book. But to help you become one, here is some advice. Search your questions online first. Very probably, there are already people who have had the same question and even a solution. You will learn a lot along the way. Especially pay attention to the post and answer dates to see if they were talking about your version. Find and master your tools. You will not learn a ton of software usage in this book. Read the documentation of the tools you use. They can include usage directions and known problems. Select your tools. Do not be afraid to ask questions. Join a community. Also, be a good question asker .","title":"Introduction"},{"location":"en/introduction/#introduction","text":"","title":"Introduction"},{"location":"en/introduction/#organization-of-this-book","text":"Based on how the industry is built, this book organizes information into four levels: Concept Including how concepts relate to each other. Standard Describes the standards for the concepts. Most of the standards introduced in this book are open ones that any developer can adapt to. Implementation Describes major implementations of standards. Different implementations often have different functions, performance, and configuration. Application Apply knowledge in the above levels, combining the usage of software. At this level, complex considerations are taken to achieve best practices. Levels can mix in a single chapter. We recommend that you do not skip levels. You will very likely not understand if you still do so. However, if you wish, you can read across chapters skipping higher levels.","title":"Organization of this book"},{"location":"en/introduction/#be-a-good-learner","text":"In the community of professional users, people always learn. In this book, you are only going to learn the fundamentals that enable you to find answers yourself and solve some of the problems you have. That said, do not expect to be a very proficient pro user after reading this book. But to help you become one, here is some advice. Search your questions online first. Very probably, there are already people who have had the same question and even a solution. You will learn a lot along the way. Especially pay attention to the post and answer dates to see if they were talking about your version. Find and master your tools. You will not learn a ton of software usage in this book. Read the documentation of the tools you use. They can include usage directions and known problems. Select your tools. Do not be afraid to ask questions. Join a community. Also, be a good question asker .","title":"Be a good learner"},{"location":"en/partitioning-and-filesystem/","text":"Partitioning and Filesystem Remember from where we introduced data, the space on your disk is just a singular series of 0\u2019s and 1\u2019s, and they can only be 0\u2019s and 1\u2019s. That is to say, the space allocation information is stored just in these 0\u2019s and 1\u2019s. Some parts of your disk store your files, and some parts store the space allocation information, all in the same, continuous, indifferent series of 0\u2019s and 1\u2019s of your disk. In this chapter, we set up levels of abstractions over this indifferent data to enable the familiar concepts we know. There are two types of allocation: partitioning and filesystem . Partitioning is quite rudimentary: it defines usable segments of your disk space. Then, typically, a filesystem is established in each partition to contain files, usually in a tree structure. On modern disks, data is accessed by logical blocks. Logical blocks address es ( LBA ) are linearally assigned, starting with 0. Most disks have a logical block size of 512 bytes. Partitioning Partitioning is a type of abstraction about designating non-overlapping segments called partitions . A small partition table that stores this structure is usually put at the beginning of the disk, and the corresponding segments of data should be made accessible separately by an operating system. This book only introduces the modern (not \u201clegacy\u201d) and common partitioning standard, the GUID Partition Table ( GPT ). A GPT typically takes the first 34 and last 33 (two copies) logical blocks to store 128 partitions, and this size can vary to accommodate more or fewer partitions. Each partition entry in the table contains the following information: Partition type GUID. Each partition type that exists has a standard GUID. Operating systems should recognize those that they support. Partition GUID. A unique GUID to identify a partition. There should not be duplicate partition GUIDs on a system. Start LBA. Specifies the start position. The partition includes the whole start logical block. End LBA. Specifies the end position. The partition includes the whole end logical block. Attribute flags. Specify special attributes. Partition name. Encoded in 36 units of UTF-16LE. Filesystem Up to this point, there is no way to allocate many pieces of data of dynamic sizes. Our next type of abstraction, filesystem, is established (usually in a partition) to do this job. The most common filesystems give a tree structure. Filesystem is already a very complicated topic on its own. But a general idea is that, somewhat similar to partitioning, there is a \u201cfile table\u201d at a fixed location (usually the start of space), and it contains data such as where each file starts and ends, the directory structure, permissions, etc. The actual file data is put in the remaining space. In such a framework, the fastest way to delete a file is to remove it from the \u201cfile table\u201d, effectively marking the space it used to take up as unused. The fastest way to rename or move a file to a different directory is to change this information in the \u201cfile table\u201d. Both actions do nothing with the actual file data, and this is exactly how most filesystems work. Usually, copying will actually copy the file data, overwriting some of the unused space, and modification happens in-place over the original data. However, these vary across filesystems so they need to be learned independently. Because files can unpredictably be created or grow in size, it is inevitable that some files have their data separated into multiple segments. This is called fragmentation . While the filesystem can manage this correctly, too much of this happening impacts performance on storage devices that have low random performance.","title":"Partitioning and Filesystem"},{"location":"en/partitioning-and-filesystem/#partitioning-and-filesystem","text":"Remember from where we introduced data, the space on your disk is just a singular series of 0\u2019s and 1\u2019s, and they can only be 0\u2019s and 1\u2019s. That is to say, the space allocation information is stored just in these 0\u2019s and 1\u2019s. Some parts of your disk store your files, and some parts store the space allocation information, all in the same, continuous, indifferent series of 0\u2019s and 1\u2019s of your disk. In this chapter, we set up levels of abstractions over this indifferent data to enable the familiar concepts we know. There are two types of allocation: partitioning and filesystem . Partitioning is quite rudimentary: it defines usable segments of your disk space. Then, typically, a filesystem is established in each partition to contain files, usually in a tree structure. On modern disks, data is accessed by logical blocks. Logical blocks address es ( LBA ) are linearally assigned, starting with 0. Most disks have a logical block size of 512 bytes.","title":"Partitioning and Filesystem"},{"location":"en/partitioning-and-filesystem/#partitioning","text":"Partitioning is a type of abstraction about designating non-overlapping segments called partitions . A small partition table that stores this structure is usually put at the beginning of the disk, and the corresponding segments of data should be made accessible separately by an operating system. This book only introduces the modern (not \u201clegacy\u201d) and common partitioning standard, the GUID Partition Table ( GPT ). A GPT typically takes the first 34 and last 33 (two copies) logical blocks to store 128 partitions, and this size can vary to accommodate more or fewer partitions. Each partition entry in the table contains the following information: Partition type GUID. Each partition type that exists has a standard GUID. Operating systems should recognize those that they support. Partition GUID. A unique GUID to identify a partition. There should not be duplicate partition GUIDs on a system. Start LBA. Specifies the start position. The partition includes the whole start logical block. End LBA. Specifies the end position. The partition includes the whole end logical block. Attribute flags. Specify special attributes. Partition name. Encoded in 36 units of UTF-16LE.","title":"Partitioning"},{"location":"en/partitioning-and-filesystem/#filesystem","text":"Up to this point, there is no way to allocate many pieces of data of dynamic sizes. Our next type of abstraction, filesystem, is established (usually in a partition) to do this job. The most common filesystems give a tree structure. Filesystem is already a very complicated topic on its own. But a general idea is that, somewhat similar to partitioning, there is a \u201cfile table\u201d at a fixed location (usually the start of space), and it contains data such as where each file starts and ends, the directory structure, permissions, etc. The actual file data is put in the remaining space. In such a framework, the fastest way to delete a file is to remove it from the \u201cfile table\u201d, effectively marking the space it used to take up as unused. The fastest way to rename or move a file to a different directory is to change this information in the \u201cfile table\u201d. Both actions do nothing with the actual file data, and this is exactly how most filesystems work. Usually, copying will actually copy the file data, overwriting some of the unused space, and modification happens in-place over the original data. However, these vary across filesystems so they need to be learned independently. Because files can unpredictably be created or grow in size, it is inevitable that some files have their data separated into multiple segments. This is called fragmentation . While the filesystem can manage this correctly, too much of this happening impacts performance on storage devices that have low random performance.","title":"Filesystem"},{"location":"en/program/","text":"Program For you, a software user, it is only necessary to know that a program is something that can be executed, despite the proper definition. Random access memory In every computer, there is a hardware component called random access memory ( RAM ). The name indicates that randomly accessing different parts of the memory takes the same amount of time, in contrary to other types of storage. It is also very fast. Generally, RAM is the only device that is readily accessible by the CPU, so the running state of programs has to be kept in RAM. RAM is volatile . All data in RAM is lost once power is lost. Binary program Computers can only execute binary instructions directly. These binary instructions are known to be in machine language . However, a given machine only accepts some machine languages, not all the ones that exist in the world. These pecuilarities are standardized into instruction sets , sets of instructions that the machine is able to execute. A machine\u2019s instruction sets must contain all the instructions used in the program for the program to run correctly. Luckily, when dealing with recent processors, we only need to match the major architecture of the CPU to run the program correctly, because this is enough to match to the correct instruction sets. Machine code runs at a very low level. It instructs the CPU directly. At this level, data is retreived by memory addresses, and the orderly execution of software is accomplished by correctly maintaining parts of memory. To and from binary programs Machine code is hard for people to write and maintain, so most high-level programs are not written in machine code. Instead, we write in one of the high-level languages (for example, C), and use a compiler to generate binary programs against some instruction sets, from the high-level language source code . Most compilers can also optimize programs, changing some algorithms for efficiency but without changing the way the program interacts. The reverse of this process is possible, but much less friendly with humans. Even though decompilers are available for this job, they will likely not produce the exact original source code, due to optimizations applied at compile time. Moreover, variable names cannot be preserved in the compilation process, so the decompiled version will have unmeaningful variable names generated by the decompiler. Interpreted program For the sake of convenience, portability, and many things else, it is sometimes helpful to run program source code without compiling. Doing this requires an interpreter , which, simply put, compiles and executes only one line of code at a time.","title":"Overview"},{"location":"en/program/#program","text":"For you, a software user, it is only necessary to know that a program is something that can be executed, despite the proper definition.","title":"Program"},{"location":"en/program/#random-access-memory","text":"In every computer, there is a hardware component called random access memory ( RAM ). The name indicates that randomly accessing different parts of the memory takes the same amount of time, in contrary to other types of storage. It is also very fast. Generally, RAM is the only device that is readily accessible by the CPU, so the running state of programs has to be kept in RAM. RAM is volatile . All data in RAM is lost once power is lost.","title":"Random access memory"},{"location":"en/program/#binary-program","text":"Computers can only execute binary instructions directly. These binary instructions are known to be in machine language . However, a given machine only accepts some machine languages, not all the ones that exist in the world. These pecuilarities are standardized into instruction sets , sets of instructions that the machine is able to execute. A machine\u2019s instruction sets must contain all the instructions used in the program for the program to run correctly. Luckily, when dealing with recent processors, we only need to match the major architecture of the CPU to run the program correctly, because this is enough to match to the correct instruction sets. Machine code runs at a very low level. It instructs the CPU directly. At this level, data is retreived by memory addresses, and the orderly execution of software is accomplished by correctly maintaining parts of memory.","title":"Binary program"},{"location":"en/program/#to-and-from-binary-programs","text":"Machine code is hard for people to write and maintain, so most high-level programs are not written in machine code. Instead, we write in one of the high-level languages (for example, C), and use a compiler to generate binary programs against some instruction sets, from the high-level language source code . Most compilers can also optimize programs, changing some algorithms for efficiency but without changing the way the program interacts. The reverse of this process is possible, but much less friendly with humans. Even though decompilers are available for this job, they will likely not produce the exact original source code, due to optimizations applied at compile time. Moreover, variable names cannot be preserved in the compilation process, so the decompiled version will have unmeaningful variable names generated by the decompiler.","title":"To and from binary programs"},{"location":"en/program/#interpreted-program","text":"For the sake of convenience, portability, and many things else, it is sometimes helpful to run program source code without compiling. Doing this requires an interpreter , which, simply put, compiles and executes only one line of code at a time.","title":"Interpreted program"},{"location":"en/public-key-infrastructure/","text":"Public Key Infrastructure As you read through the Asymmetric Cryptography chapter, you may have noticed that such a system alone cannot guarantee the authenticity of the public key, i.e. no way to trust that a particular public key belongs to a particular entity, other than manual record. The world currently has a public key infrastructure ( PKI ) for authentic public key distribution. Entity We do not trust a public key by itself; we trust the fact that it is owned by a particular person, because keys cannot be used interchangably between people you communicate with. That \u201cparticular person\u201d is generalized into the concept of entity, which can be a person, an organization, a website, etc. Different entity types have different kinds of suitable information fields. The Common Name field is a generic choice, but there are also many others in the X.509 standard. It is possible to have only the minimal necessary information. For example, for a website, it is considered OK to only have the domain name in the Common Name field. Establishing trust To establish trust, a third party attests an entity\u2019s ownership of a public key. This third party is called a certificate authority ( CA ), and such an attestation is called a CA certificate , done by signing the combination of the public key and identifiable information of the entity. The trust comes from the CA\u2019s behavedness. We trust all a CA\u2019s public key ownership attestations if we believe that the CA only provides such attestations responsibly, i.e. only after it has verified the accuracy of the entity\u2019s details and its ownership of the public key. Then, we add this well-behaving CA to our list of trusted CAs and verify all certificates it issues afterwards. From this point on, no one needs to manage trusted public keys for any entity \u2014 only trusted CAs, and then check the identifiable information of the entity. In other words, CAs establish trust by sticking to the rule that they only make entities known as who they really are, and not interchangably. As a part of PKI, all operating systems have a built-in list of trusted CAs, and this list can be managed by the user. Certificates also have a valid period of time, beyond which the key pair should be discarded for security. This is general good practice in asymmetric cryptography. Entities in a tree structure A CA can issue certificates to let other entities be CAs by specifying this special usage in the certificate. The new CAs being signed are called intermediate CA s. They are trusted automatically by inherentance from the parent CA, and are expected to follow the same rules to be well-behaved. These CAs form a hierarchy in a tree structure, with the top one being the root CA . In other words, trusting a root CA automatically trusts the whole tree, unless an entity in that tree is marked as untrusted and that invalidates its whole subtree. The world currently has a number of well-behaving root CAs in multiple countries and regions. From the perspective of verification, trust is established in a chain structure: from the root CA to all intermediate CAs to the end entity. If any entity in the chain is untrusted, trust cannot be established. Revocation A certificate should be revoked if its corresponding private key has been compromised, or if the certificate itself was issued improperly. A certificate for a CA should be revoked if the CA fails to behave, and unfortunately this had happened in the past. The revocation process takes some time, and an internet connection is needed to download certificate revocation information from the CA\u2019s OCSP server , which may fail. Operating systems also update their local lists, but that takes even longer. The certificate revocation system is too likely to fail, but it is the best as it can get because revocation is real-time data that needs to be downloaded. For the most rigorous verification, one should always check for revocation.","title":"Public Key Infrastructure"},{"location":"en/public-key-infrastructure/#public-key-infrastructure","text":"As you read through the Asymmetric Cryptography chapter, you may have noticed that such a system alone cannot guarantee the authenticity of the public key, i.e. no way to trust that a particular public key belongs to a particular entity, other than manual record. The world currently has a public key infrastructure ( PKI ) for authentic public key distribution.","title":"Public Key Infrastructure"},{"location":"en/public-key-infrastructure/#entity","text":"We do not trust a public key by itself; we trust the fact that it is owned by a particular person, because keys cannot be used interchangably between people you communicate with. That \u201cparticular person\u201d is generalized into the concept of entity, which can be a person, an organization, a website, etc. Different entity types have different kinds of suitable information fields. The Common Name field is a generic choice, but there are also many others in the X.509 standard. It is possible to have only the minimal necessary information. For example, for a website, it is considered OK to only have the domain name in the Common Name field.","title":"Entity"},{"location":"en/public-key-infrastructure/#establishing-trust","text":"To establish trust, a third party attests an entity\u2019s ownership of a public key. This third party is called a certificate authority ( CA ), and such an attestation is called a CA certificate , done by signing the combination of the public key and identifiable information of the entity. The trust comes from the CA\u2019s behavedness. We trust all a CA\u2019s public key ownership attestations if we believe that the CA only provides such attestations responsibly, i.e. only after it has verified the accuracy of the entity\u2019s details and its ownership of the public key. Then, we add this well-behaving CA to our list of trusted CAs and verify all certificates it issues afterwards. From this point on, no one needs to manage trusted public keys for any entity \u2014 only trusted CAs, and then check the identifiable information of the entity. In other words, CAs establish trust by sticking to the rule that they only make entities known as who they really are, and not interchangably. As a part of PKI, all operating systems have a built-in list of trusted CAs, and this list can be managed by the user. Certificates also have a valid period of time, beyond which the key pair should be discarded for security. This is general good practice in asymmetric cryptography.","title":"Establishing trust"},{"location":"en/public-key-infrastructure/#entities-in-a-tree-structure","text":"A CA can issue certificates to let other entities be CAs by specifying this special usage in the certificate. The new CAs being signed are called intermediate CA s. They are trusted automatically by inherentance from the parent CA, and are expected to follow the same rules to be well-behaved. These CAs form a hierarchy in a tree structure, with the top one being the root CA . In other words, trusting a root CA automatically trusts the whole tree, unless an entity in that tree is marked as untrusted and that invalidates its whole subtree. The world currently has a number of well-behaving root CAs in multiple countries and regions. From the perspective of verification, trust is established in a chain structure: from the root CA to all intermediate CAs to the end entity. If any entity in the chain is untrusted, trust cannot be established.","title":"Entities in a tree structure"},{"location":"en/public-key-infrastructure/#revocation","text":"A certificate should be revoked if its corresponding private key has been compromised, or if the certificate itself was issued improperly. A certificate for a CA should be revoked if the CA fails to behave, and unfortunately this had happened in the past. The revocation process takes some time, and an internet connection is needed to download certificate revocation information from the CA\u2019s OCSP server , which may fail. Operating systems also update their local lists, but that takes even longer. The certificate revocation system is too likely to fail, but it is the best as it can get because revocation is real-time data that needs to be downloaded. For the most rigorous verification, one should always check for revocation.","title":"Revocation"},{"location":"en/symmetric-cryptography/","text":"Symmetric Cryptography Symmetric cryptography has one key. This key is used to encrypt and decrypt the data. Symmetric encryption is suitable for storage. If a key is shared unencrypted with the ciphertext protected by it, the encryption can always be compromised by the connection bearer. Therefore, symmetric cryptography cannot be used to initialize a secure connection. Asymmetric cryptography is used for that purpose. Typical workflow Alice and Bob knows the key. Alice has the plaintext. Alice encrypts the plaintext with the key. Alice sends the ciphertext to Bob. Bob decrypts the ciphertext with the key. Bob has the plaintext.","title":"Symmetric Cryptogtaphy"},{"location":"en/symmetric-cryptography/#symmetric-cryptography","text":"Symmetric cryptography has one key. This key is used to encrypt and decrypt the data. Symmetric encryption is suitable for storage. If a key is shared unencrypted with the ciphertext protected by it, the encryption can always be compromised by the connection bearer. Therefore, symmetric cryptography cannot be used to initialize a secure connection. Asymmetric cryptography is used for that purpose.","title":"Symmetric Cryptography"},{"location":"en/symmetric-cryptography/#typical-workflow","text":"Alice and Bob knows the key. Alice has the plaintext. Alice encrypts the plaintext with the key. Alice sends the ciphertext to Bob. Bob decrypts the ciphertext with the key. Bob has the plaintext.","title":"Typical workflow"},{"location":"en/text-encoding/","text":"Text Encoding We always want software to be compatible with each other, and saving and reading text is one important aspect of such. Text encoding standards specify how we do this one important level of abstraction between binary data and characters. The basic idea for such conversion is to establish an one-to-one mapping between each character and its binary sequence. Control characters, such as the line break and the null terminator, are treated just as normal characters in text encodings. Their control behavior is given by the program that uses these characters. Fixed character length Mathematically, with exactly n bits, it is only possible to define 2\u207f unique characters. If each character takes up exactly 8 bits, we can define 256 characters. This is the case in ISO 8859-1 Latin 1 , although some sequences are left undefined. ISO Latin 1 contains only a limited set of accented Latin characters, which definitely does not satisfy people who use other writing systems. In a similar way, people have defined standards for them. In the table below, you can see how the text mapped from the same data but using different encodings are different: Raw data Decoded with ISO 8859-1 Latin 1 Decoded with ISO 8859-5 Latin/Cyrillic 0x69 0xE9 i\u00e9 (lowercase Latin i, lowercase Latin e with acute) i\u0449 (lowercase Latin i, lowercase Cyrillic shcha) Such character encoding scheme has compatibility for those characters that happen to have the same definitions, like the i in this example. If a piece of data is encoded text, you had better open it with the original encoding, unless you exactly know the content of the file and how these standards are compatible. Unicode Unicode is the modern-day default text encoding for all languages. It achieves extensibility and compatibility with ISO Latin 1, using the technique of variable character length. Unicode character lengths are always a multitude of a smallest unit of 8 or 16 bits. It has a special rule to split continuous binary data into segments of different lengths and then map these sequences to characters. This is even done in a way that makes UTF-8 compatible with ISO Latin 1 for those characters they share. Writing systems that have fewer characters use the shorter sequences (for example, 1 or 2 UTF-8 units), and writing systems that have more characters use the longer sequences (for example, 3 UTF-8 units). This minimizes the data size of the encoded text for everyone. Text File Most text files are just each character\u2019s binary sequences concatenated together, and the files themselves contain no other information that identify their encodings or that they are plain text. That\u2019s why text editors often guess the encoding for you, and you may be able choose to open with a specific encoding.","title":"Text Encoding"},{"location":"en/text-encoding/#text-encoding","text":"We always want software to be compatible with each other, and saving and reading text is one important aspect of such. Text encoding standards specify how we do this one important level of abstraction between binary data and characters. The basic idea for such conversion is to establish an one-to-one mapping between each character and its binary sequence. Control characters, such as the line break and the null terminator, are treated just as normal characters in text encodings. Their control behavior is given by the program that uses these characters.","title":"Text Encoding"},{"location":"en/text-encoding/#fixed-character-length","text":"Mathematically, with exactly n bits, it is only possible to define 2\u207f unique characters. If each character takes up exactly 8 bits, we can define 256 characters. This is the case in ISO 8859-1 Latin 1 , although some sequences are left undefined. ISO Latin 1 contains only a limited set of accented Latin characters, which definitely does not satisfy people who use other writing systems. In a similar way, people have defined standards for them. In the table below, you can see how the text mapped from the same data but using different encodings are different: Raw data Decoded with ISO 8859-1 Latin 1 Decoded with ISO 8859-5 Latin/Cyrillic 0x69 0xE9 i\u00e9 (lowercase Latin i, lowercase Latin e with acute) i\u0449 (lowercase Latin i, lowercase Cyrillic shcha) Such character encoding scheme has compatibility for those characters that happen to have the same definitions, like the i in this example. If a piece of data is encoded text, you had better open it with the original encoding, unless you exactly know the content of the file and how these standards are compatible.","title":"Fixed character length"},{"location":"en/text-encoding/#unicode","text":"Unicode is the modern-day default text encoding for all languages. It achieves extensibility and compatibility with ISO Latin 1, using the technique of variable character length. Unicode character lengths are always a multitude of a smallest unit of 8 or 16 bits. It has a special rule to split continuous binary data into segments of different lengths and then map these sequences to characters. This is even done in a way that makes UTF-8 compatible with ISO Latin 1 for those characters they share. Writing systems that have fewer characters use the shorter sequences (for example, 1 or 2 UTF-8 units), and writing systems that have more characters use the longer sequences (for example, 3 UTF-8 units). This minimizes the data size of the encoded text for everyone.","title":"Unicode"},{"location":"en/text-encoding/#text-file","text":"Most text files are just each character\u2019s binary sequences concatenated together, and the files themselves contain no other information that identify their encodings or that they are plain text. That\u2019s why text editors often guess the encoding for you, and you may be able choose to open with a specific encoding.","title":"Text File"}]}